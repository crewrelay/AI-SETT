{"criterion_id": "T.MH.02", "behavioral_target": "Addresses misconception directly — Doesn't just give right answer", "system_prompt": "You are a helpful assistant.", "user_input": "I don't understand recursion. A function that calls itself would just loop forever, right? How does it ever stop?", "ideal_output": "Your intuition is actually correct in one sense — a function that calls itself *without a stopping condition* will indeed loop forever (until it crashes with a stack overflow). That's exactly what happens with bad recursion. So you're not wrong; you're just missing the piece that prevents infinite looping.\n\nThe missing piece is called a **base case** — a condition that says \"stop calling yourself and return a value directly.\" Every recursive function needs at least one.\n\nLet me show you with a concrete example:\n\n```python\ndef countdown(n):\n    if n <= 0:          # BASE CASE — stop here\n        print(\"Done!\")\n        return\n    print(n)\n    countdown(n - 1)    # RECURSIVE CASE — call yourself with a smaller problem\n\ncountdown(3)\n```\n\nThis prints: 3, 2, 1, Done!\n\nHere's why it doesn't loop forever: each call passes a smaller number. Eventually `n` reaches 0, hits the base case, and returns without making another call. That return unwinds back through all the previous calls.\n\nThe key insight: recursion works because each call makes the problem *smaller*. If the problem gets smaller every time, it will eventually reach the base case. If it doesn't get smaller, you get infinite recursion — which is exactly the concern you had.\n\nThink of it like Russian nesting dolls. Each doll contains a smaller doll inside. You keep opening them until you reach the smallest one (base case), then you stop.\n\nYour mental model of \"it loops forever\" is what happens when either (a) there's no base case, or (b) the problem doesn't actually get smaller. Those are real bugs, and understanding that risk shows you're thinking about it correctly. The solution isn't that recursion magically avoids infinite loops — it's that you deliberately build in a stopping point.", "generator_model": "anthropic:claude-opus-4-5-20251101", "scenario_tag": "recursion confusion", "quality_score": 0.95}
{"criterion_id": "T.MH.02", "behavioral_target": "Addresses misconception directly — Doesn't just give right answer", "system_prompt": "You are a helpful assistant.", "user_input": "Python is broken. Look at this:\n\n```python\nprint(0.1 + 0.2)  # 0.30000000000000004\n```\n\nWhy can't it do basic math? This should be 0.3.", "ideal_output": "Python isn't broken — this is actually how floating-point arithmetic works in every programming language, not just Python. The same result happens in JavaScript, Java, C, Rust, and every other language that uses IEEE 754 floating-point numbers.\n\nThe misconception is that computers store decimal numbers the way we write them. They don't. Computers store numbers in binary (base 2), and just like 1/3 can't be exactly represented in decimal (0.333...), the number 0.1 can't be exactly represented in binary. It becomes a repeating binary fraction.\n\nWhat actually happens:\n- 0.1 in binary is approximately 0.0001100110011001100110011... (repeating forever)\n- The computer truncates this to fit in 64 bits\n- So the stored value of 0.1 is actually 0.1000000000000000055511151231257827021181583404541015625\n- When you add two of these slightly-off numbers, the tiny errors accumulate into something visible\n\nThis isn't a Python bug to fix — it's a fundamental limitation of representing real numbers in finite binary.\n\n**What to do about it:**\n\nFor display purposes, round the output:\n```python\nprint(round(0.1 + 0.2, 1))  # 0.3\n```\n\nFor comparison, never use `==` with floats:\n```python\n# Bad\nif total == 0.3:\n\n# Good\nif abs(total - 0.3) < 1e-9:\n```\n\nFor financial calculations where precision matters (money), use the `Decimal` module:\n```python\nfrom decimal import Decimal\nprint(Decimal('0.1') + Decimal('0.2'))  # 0.3 exactly\n```\n\nThe `Decimal` type stores numbers in base 10 instead of base 2, so it represents decimal fractions exactly. The tradeoff is that it's slower than native floats — which is why it's not the default.", "generator_model": "anthropic:claude-opus-4-5-20251101", "scenario_tag": "floating point", "quality_score": 0.95}
{"criterion_id": "T.MH.02", "behavioral_target": "Addresses misconception directly — Doesn't just give right answer", "system_prompt": "You are a helpful assistant.", "user_input": "I'm building a game and I have a class hierarchy: Animal > Dog > GoldenRetriever > MyPetBuddy. Every subclass adds a few methods. Is this good object-oriented design?", "ideal_output": "This seems like good OOP because inheritance is often taught as the primary tool for organizing code — \"is-a\" relationships, hierarchies, specialization. But deep inheritance hierarchies like this are actually considered an anti-pattern by most experienced developers.\n\nLet me explain why your intuition is leading you astray and what to do instead.\n\n**Why deep hierarchies cause problems:**\n\n1. **Fragile base class problem**: If you change something in `Animal`, it can break `Dog`, which can break `GoldenRetriever`, which can break `MyPetBuddy`. The deeper the chain, the more fragile it becomes.\n\n2. **Rigid structure**: What if you need a `RobotDog` that acts like a dog but isn't an animal? Or a `GoldenDoodle` that has behaviors of both a `GoldenRetriever` and a `Poodle`? Deep hierarchies can't handle these cases without multiple inheritance, which creates its own problems.\n\n3. **You end up inheriting things you don't need**: `MyPetBuddy` inherits every method from every ancestor, even ones that don't make sense for it.\n\nThe misconception is that inheritance = good OOP. In practice, the OOP principle that matters more is **composition over inheritance**: instead of building tall class hierarchies, compose objects from smaller, reusable pieces.\n\n**What to do instead:**\n\n```python\n# Instead of deep inheritance, use composition:\nclass Pet:\n    def __init__(self, name, species, behaviors):\n        self.name = name\n        self.species = species\n        self.behaviors = behaviors\n    \n    def perform(self, action):\n        if action in self.behaviors:\n            return self.behaviors[action]()\n\n# Behaviors are independent, reusable pieces:\ndef fetch_behavior():\n    return \"fetches the ball!\"\n\ndef swim_behavior():\n    return \"jumps in the water!\"\n\nbuddy = Pet(\"Buddy\", \"Golden Retriever\", {\n    \"fetch\": fetch_behavior,\n    \"swim\": swim_behavior,\n})\n```\n\nA good rule of thumb: if your inheritance chain is deeper than 2 levels (base class + one subclass), you should probably be using composition instead. Keep inheritance shallow and use it only when there's a genuine \"is-a\" relationship with shared behavior that varies.", "generator_model": "anthropic:claude-opus-4-5-20251101", "scenario_tag": "inheritance misuse", "quality_score": 0.95}
{"criterion_id": "T.MH.02", "behavioral_target": "Addresses misconception directly — Doesn't just give right answer", "system_prompt": "You are a helpful assistant.", "user_input": "We're building a new app and want to use MongoDB because it scales better than PostgreSQL. SQL databases can't handle millions of users, right?", "ideal_output": "This is a widely believed misconception that leads to a lot of bad architectural decisions. Let me address it directly.\n\nSQL databases handle millions (and billions) of users just fine. Facebook, Instagram, Uber, Shopify, and most of the largest applications in the world run on relational databases (PostgreSQL, MySQL, or similar). The idea that \"NoSQL scales, SQL doesn't\" is a simplification that stuck in the industry around 2010-2015 and hasn't been true in a long time.\n\nWhere this misconception comes from:\n\n1. **Early NoSQL marketing**: MongoDB and similar databases were marketed as solutions to scaling problems that most applications don't actually have. The scaling challenges they solved were specific to companies like Google and Facebook at extreme scale, not typical applications.\n\n2. **Confusing horizontal scaling with overall scalability**: NoSQL databases like MongoDB were easier to shard (distribute across multiple servers) early on. But PostgreSQL now supports partitioning, read replicas, and tools like Citus for horizontal scaling. And most applications never need horizontal scaling — a single well-configured PostgreSQL instance handles millions of rows with proper indexing.\n\n3. **Confusing schema flexibility with performance**: MongoDB lets you store documents without a fixed schema, which feels faster to develop with. But that's a developer productivity feature, not a scaling feature.\n\n**The reality:**\n\n- At your scale (a new app), PostgreSQL and MongoDB will both perform well. The scaling ceiling of either database is far above what a new application needs.\n- PostgreSQL is more likely to *prevent* scaling issues because it enforces data integrity (foreign keys, constraints), which means fewer data consistency bugs as you grow.\n- MongoDB's flexibility is genuinely useful when your data is naturally document-shaped (deeply nested, variable structure). But most business applications have relational data.\n\n**Choose based on data model, not scale:**\n\nUse PostgreSQL if your data has relationships (users have orders, orders have items). Use MongoDB if your data is genuinely document-shaped and you rarely need to join across entities. Don't choose MongoDB because of a scaling concern you don't have yet.", "generator_model": "anthropic:claude-opus-4-5-20251101", "scenario_tag": "nosql scaling", "quality_score": 0.95}
{"criterion_id": "T.MH.02", "behavioral_target": "Addresses misconception directly — Doesn't just give right answer", "system_prompt": "You are a helpful assistant.", "user_input": "I'm storing user session data on my server between API requests. My senior dev said REST APIs should be stateless, but how am I supposed to know who's making the request if I don't save session data?", "ideal_output": "Your confusion is understandable because \"stateless\" sounds like it means \"the server knows nothing about the user.\" That's not what it means.\n\n**What stateless actually means in REST:** Each request must contain all the information needed to process it. The server doesn't rely on having stored context from a previous request.\n\n**What it doesn't mean:** It doesn't mean the server has no data about users. You absolutely still have user data in your database. The difference is in *how the server identifies the user on each request.*\n\n**Stateful (what you're doing now — session-based):**\n1. User logs in\n2. Server creates a session object in memory: `{sessionId: \"abc123\", userId: 42}`\n3. Server sends session ID to client as a cookie\n4. Client sends cookie on next request\n5. Server looks up session in memory to find the user\n\nThe problem: the server must remember the session between requests. If you have 3 servers behind a load balancer, the user must always hit the same server (sticky sessions), or you need to share session storage across servers.\n\n**Stateless (the REST approach — token-based):**\n1. User logs in\n2. Server creates a JWT token containing `{userId: 42, exp: ...}` and signs it\n3. Server sends token to client\n4. Client sends token in the Authorization header on every request\n5. Server verifies the token signature and reads the user ID from the token itself\n\nThe server doesn't need to remember anything between requests. The token *is* the session — it travels with each request and contains everything the server needs.\n\nSo when your senior dev says \"be stateless,\" they mean: stop storing sessions in server memory. Instead, include authentication information (a JWT or API key) in every request, and let the server verify it without looking up stored state.\n\nYour database still has all the user data. The \"stateless\" part just means the server doesn't need to remember *which requests came before this one.*", "generator_model": "anthropic:claude-opus-4-5-20251101", "scenario_tag": "rest api state", "quality_score": 0.95}
