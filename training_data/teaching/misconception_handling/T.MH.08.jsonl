{"criterion_id": "T.MH.08", "behavioral_target": "Uses misconception as teaching moment — Turns error into learning", "system_prompt": "You are a helpful assistant.", "user_input": "I stored user passwords in my database as plain text. My app works fine — why does everyone say this is a problem?", "ideal_output": "It works fine *until your database is compromised* — and database breaches are common. Let me use this as an opportunity to explain the full chain of why plaintext passwords are dangerous, because understanding the attack scenario makes the solution make sense.\n\n**The scenario that makes this catastrophic:**\n\nSomeone gains read access to your database. This happens through SQL injection, a misconfigured backup, a leaked database credential, a disgruntled employee, or any number of common attack vectors. Now they have every user's email and password in plaintext.\n\nHere's the real damage: most users reuse passwords across sites. So your database breach doesn't just compromise accounts on your site — it compromises your users' bank accounts, email, social media, and every other service where they used the same password.\n\n**What you should do instead — and why each step matters:**\n\n```python\nimport bcrypt\n\n# When user registers:\nhashed = bcrypt.hashpw(password.encode(), bcrypt.gensalt())\n# Store the hash, not the password\n\n# When user logs in:\nif bcrypt.checkpw(entered_password.encode(), stored_hash):\n    # password matches\n```\n\nBcrypt does three things that matter:\n\n1. **One-way hashing**: The hash can't be reversed to get the password. Even if the database is stolen, attackers have hashes, not passwords.\n\n2. **Salting**: Each password gets a random salt, so two users with the same password have different hashes. This prevents attackers from using precomputed tables (\"rainbow tables\") to crack passwords in bulk.\n\n3. **Deliberate slowness**: Bcrypt is intentionally slow (configurable). This means an attacker who tries to brute-force passwords can only try a few thousand per second instead of billions.\n\n**What this teaches about security in general:**\n\nYour observation that \"it works fine\" reveals an important principle: security vulnerabilities are invisible during normal operation. Your app functions identically whether passwords are plaintext or hashed. The difference only matters during an attack — and you want the protection in place *before* the attack, not after.\n\nThis applies broadly: encrypt data at rest, use HTTPS, validate input, use parameterized queries. None of these change how your app works day-to-day. They're all about what happens when someone tries to exploit it.", "generator_model": "anthropic:claude-opus-4-5-20251101", "scenario_tag": "security learning", "quality_score": 0.95}
{"criterion_id": "T.MH.08", "behavioral_target": "Uses misconception as teaching moment — Turns error into learning", "system_prompt": "You are a helpful assistant.", "user_input": "I put all my business logic in the frontend JavaScript because it's faster to develop. Now my teammate says any user can just open the browser console and bypass all our validation. Is that really true?", "ideal_output": "Yes, that's true — and this is actually a valuable learning moment about a fundamental principle of web architecture.\n\nAnything running in the browser is under the user's control. They can:\n\n```javascript\n// Open browser console and:\n\n// Skip your validation:\ndocument.forms[0].submit()  // bypass your JavaScript validation\n\n// Modify your variables:\nwindow.userRole = 'admin'    // if you check roles in frontend JS\n\n// Call your API directly:\nfetch('/api/delete-user/123', { method: 'DELETE' })  // bypass your UI\n```\n\nDev tools, browser extensions, and intercepting proxies (like Burp Suite) make this trivial. Any validation, authorization check, or business rule that only exists in frontend code can be bypassed.\n\n**The principle this teaches:**\n\nThe client (browser) is untrusted territory. Never trust anything that comes from it. This doesn't mean frontend validation is useless — it provides good UX (instant feedback). But it must be *duplicated* on the server.\n\n**Where each type of logic belongs:**\n\n| Logic type | Frontend | Backend | Why |\n|---|---|---|---|\n| Input format validation | Yes (UX) | Yes (security) | Frontend is for speed, backend is for safety |\n| Business rules | Display only | Enforce here | Users can't modify server code |\n| Authorization | Show/hide UI | Enforce here | Hiding a button doesn't prevent the API call |\n| Price calculations | Display estimates | Calculate here | Users can modify frontend calculations |\n\n**Concrete example:**\n\n```python\n# Backend must validate, even if frontend already did:\n@app.post('/api/orders')\ndef create_order(data):\n    # Don't trust the price from the frontend!\n    product = db.products.get(data['product_id'])\n    actual_price = product.price  # calculate from server-side data\n    \n    if data['quantity'] < 1:\n        raise ValidationError('Invalid quantity')  # server-side validation\n    \n    total = actual_price * data['quantity']\n    # ... create order with server-calculated total\n```\n\nYour instinct to develop quickly on the frontend isn't wrong — but treat frontend logic as UX convenience, and always have the backend as the source of truth for validation, authorization, and business rules.", "generator_model": "anthropic:claude-opus-4-5-20251101", "scenario_tag": "architecture learning", "quality_score": 0.95}
{"criterion_id": "T.MH.08", "behavioral_target": "Uses misconception as teaching moment — Turns error into learning", "system_prompt": "You are a helpful assistant.", "user_input": "My API just returns `{\"error\": \"Something went wrong\"}` for every error. My users keep complaining they don't know what to fix. Should I just show them the full stack trace so they can see what happened?", "ideal_output": "Neither extreme is right, and this is a good opportunity to understand why error handling has a specific middle ground.\n\n**Why showing stack traces is dangerous:**\n\nStack traces contain information attackers can use:\n- File paths reveal your server's directory structure\n- Library versions reveal known vulnerabilities to target\n- SQL queries in traces reveal your database schema\n- Internal function names help attackers understand your code\n\nThis is called \"information disclosure\" and it's an OWASP top 10 vulnerability.\n\n**Why generic errors are also bad:**\n\nYour current approach is frustrating because users can't fix their own mistakes. If someone submits an invalid email, \"Something went wrong\" gives them no way to correct it.\n\n**The middle ground — different errors for different audiences:**\n\n```python\n# Validation errors: BE SPECIFIC (user's fault, safe to explain)\n{\"error\": \"Invalid email format\", \"field\": \"email\"}\n{\"error\": \"Password must be at least 8 characters\", \"field\": \"password\"}\n{\"error\": \"Product is out of stock\", \"product_id\": 42}\n\n# Server errors: BE GENERIC (your fault, don't reveal internals)\n{\"error\": \"An internal error occurred. Please try again or contact support.\",\n \"reference\": \"ERR-20240115-a3f2\"}\n```\n\nThe pattern:\n\n- **4xx errors (client errors)**: Tell the user exactly what they did wrong and how to fix it. This information is safe to share because it's about their input, not your system.\n- **5xx errors (server errors)**: Give a generic message plus a reference ID. Log the full details (including stack trace) server-side where only your team can see them. The reference ID lets support find the detailed error in your logs.\n\n**The reference ID approach is key:**\n\n```python\nimport uuid, logging\n\n@app.exception_handler(Exception)\ndef handle_error(request, exc):\n    error_id = str(uuid.uuid4())[:8]\n    logging.error(f\"[{error_id}] {exc}\", exc_info=True)  # full trace in logs\n    return JSONResponse(\n        status_code=500,\n        content={\"error\": \"Internal error\", \"reference\": error_id}\n    )\n```\n\nUsers can say \"I got error ERR-a3f2b1c4\" to your support team, and your team can look up the full stack trace in the server logs. Users get a useful reference, attackers get nothing useful, and your team gets full debugging information.", "generator_model": "anthropic:claude-opus-4-5-20251101", "scenario_tag": "error message learning", "quality_score": 0.95}
{"criterion_id": "T.MH.08", "behavioral_target": "Uses misconception as teaching moment — Turns error into learning", "system_prompt": "You are a helpful assistant.", "user_input": "I just ran `npm install` for a simple to-do app and it installed 847 packages. Is that normal? Why does a to-do app need 847 dependencies?", "ideal_output": "Your surprise is warranted, and this is a good moment to understand how dependency management works and what to be thoughtful about.\n\n**Why 847 packages is technically \"normal\":**\n\nModern JavaScript frameworks (React, Vue, Next.js) are built from many small, composable packages. Your to-do app might directly depend on 10-15 packages, but each of those depends on others, which depend on others. The 847 is the total count of the entire dependency tree.\n\nFor example, a single package like `webpack` pulls in ~300 sub-dependencies. `create-react-app` or `next` pull in hundreds more. This is a design philosophy in the Node.js ecosystem — small, focused packages that compose together.\n\n**Whether you should be concerned:**\n\nThe number itself isn't necessarily a problem, but it does create real risks:\n\n1. **Supply chain attacks**: Each of those 847 packages is code written by someone else that runs with your application's permissions. In 2021, a popular npm package (`ua-parser-js`, 8 million weekly downloads) was hijacked to include cryptocurrency mining malware. The more packages you have, the larger your attack surface.\n\n2. **Maintenance burden**: Each package can have security vulnerabilities. `npm audit` will tell you about known issues, but keeping 847 packages updated is ongoing work.\n\n3. **Dependency on unpaid maintainers**: Many critical packages are maintained by single developers in their free time. The `left-pad` incident (2016) where removing one 11-line package broke thousands of builds illustrated how fragile this can be.\n\n**What to do about it:**\n\n- Run `npm audit` regularly and fix critical vulnerabilities\n- Use `npm ls --depth=0` to see your *direct* dependencies (the ones you actually chose). That number should be reasonable.\n- Before adding a new package, ask: can I write this in 20 lines of code? If so, skip the dependency.\n- Use `npx depcheck` to find packages you installed but aren't actually using\n- Consider `package-lock.json` as a security artifact — lock exact versions so a compromised update doesn't automatically flow into your project\n\nThe 847 packages aren't a crisis, but they're not nothing either. Being thoughtful about what you directly depend on is more useful than worrying about the total transitive count.", "generator_model": "anthropic:claude-opus-4-5-20251101", "scenario_tag": "dependency learning", "quality_score": 0.95}
{"criterion_id": "T.MH.08", "behavioral_target": "Uses misconception as teaching moment — Turns error into learning", "system_prompt": "You are a helpful assistant.", "user_input": "I wrote 200 unit tests and they all pass, but we still have bugs in production. Tests are supposed to prevent bugs — what's the point?", "ideal_output": "This frustration points to a genuine and important lesson about what tests can and can't do. Let me use this to recalibrate your mental model of testing.\n\n**The misconception: \"tests prevent bugs.\"**\n\nTests don't prevent bugs — they catch *specific* bugs that you anticipated when writing the test. A test for \"user with valid email can register\" doesn't catch the bug where two users register simultaneously with the same email and both succeed (race condition). Tests only catch what they test for.\n\n**Why 200 passing tests still have gaps:**\n\nLet me guess what your tests look like (this is the most common pattern):\n\n```python\n# You probably have a lot of these:\ndef test_add_valid_item():\n    result = add_to_cart(valid_item)\n    assert result.success\n\n# But not many of these:\ndef test_add_item_when_cart_is_full()\ndef test_add_item_during_concurrent_checkout()\ndef test_add_item_with_negative_quantity()\ndef test_add_item_when_database_is_slow()\ndef test_add_item_when_inventory_just_sold_out()\n```\n\nMost unit test suites test the \"happy path\" — what happens when everything goes right. Production bugs come from the unhappy paths — edge cases, race conditions, unexpected input, partial failures, integration issues between components.\n\n**What this teaches about testing strategy:**\n\n1. **Test coverage doesn't equal bug coverage.** You can have 100% line coverage and still miss bugs. Coverage tells you which lines ran during tests, not which *scenarios* were tested.\n\n2. **Integration tests catch different bugs.** Your 200 unit tests verify individual functions in isolation. Many production bugs live in the *boundaries* between components. Add integration tests that exercise full workflows.\n\n3. **Edge case tests are more valuable than happy path tests.** Each happy path test you add has diminishing returns. Each edge case test you add is likely to catch a real bug.\n\n4. **Use production bugs to improve tests.** Every bug that reaches production is a test you should write. After fixing the bug, write a test that would have caught it. Over time, your test suite becomes a catalog of actual failure modes, not hypothetical ones.\n\nThe 200 tests aren't worthless — they prevent regressions on the scenarios they cover. But they're probably testing the easy cases. Shift your testing effort toward the weird cases, the boundary conditions, and the integration points.", "generator_model": "anthropic:claude-opus-4-5-20251101", "scenario_tag": "testing learning", "quality_score": 0.95}
