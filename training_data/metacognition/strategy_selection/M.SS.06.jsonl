{"criterion_id": "M.SS.06", "behavioral_target": "Monitors progress — Checks if working", "system_prompt": "You are a helpful assistant.", "user_input": "I'm trying to get my Docker container to connect to a PostgreSQL database running on the host machine. I've tried using 'localhost' as the hostname but it can't connect. I'm on macOS. Here's my docker run command: docker run -e DB_HOST=localhost -e DB_PORT=5432 myapp:latest", "ideal_output": "The issue is that `localhost` inside a Docker container refers to the container itself, not your Mac. Let me walk through the fix and verify at each step.\n\n**Fix: Use `host.docker.internal` on macOS.**\n\n```bash\ndocker run -e DB_HOST=host.docker.internal -e DB_PORT=5432 myapp:latest\n```\n\n**But let's verify this will actually work before assuming it's solved:**\n\n**Check 1: Is PostgreSQL listening on the right interface?**\n\nPostgreSQL often defaults to listening only on 127.0.0.1. Check your postgresql.conf:\n\n```bash\ngrep listen_addresses /usr/local/var/postgresql@15/postgresql.conf\n```\n\nIf it says `listen_addresses = 'localhost'`, change to `listen_addresses = '*'` and restart PostgreSQL.\n\n**Check 2: Does pg_hba.conf allow the connection?**\n\nYou may need to add:\n```\nhost    all    all    172.17.0.0/16    md5\n```\n\n**Check 3: Verify connectivity step by step.**\n\n```bash\n# Test DNS resolution\ndocker run --rm alpine ping -c 1 host.docker.internal\n\n# Test port connectivity\ndocker run --rm alpine sh -c 'nc -zv host.docker.internal 5432'\n\n# Test PostgreSQL connection\ndocker run --rm -e PGPASSWORD=yourpass postgres:15 psql -h host.docker.internal -U youruser -d yourdb -c 'SELECT 1'\n```\n\nRun these in order. If step 1 fails, Docker Desktop issue. If step 2 fails, PostgreSQL isn't listening correctly. If step 3 fails, authentication issue. If all pass but your app fails, the problem is in your application's connection code.\n\nThis progressive verification saves time compared to changing multiple things at once.", "generator_model": "anthropic:claude-opus-4-5-20251101", "scenario_tag": "iterative debugging", "quality_score": 0.95}
{"criterion_id": "M.SS.06", "behavioral_target": "Monitors progress — Checks if working", "system_prompt": "You are a helpful assistant.", "user_input": "My React app's production build is failing with 'JavaScript heap out of memory'. The build works fine in development. Here's the error: FATAL ERROR: Ineffective mark-compacts near heap limit Allocation failed - JavaScript heap out of memory. The project has about 200 components.", "ideal_output": "Let me work through this systematically, checking after each step whether it resolves the issue or narrows the cause.\n\n**Step 1: Quick fix — increase Node's memory limit.**\n\n```bash\nexport NODE_OPTIONS=\"--max-old-space-size=8192\"\nnpm run build\n```\n\nIf the build succeeds, you've confirmed it's a memory issue (not an infinite loop) and bought time. But this is a band-aid.\n\n**Step 2: If step 1 works, measure what's consuming memory.**\n\nGenerate a build analysis to look for: a single massive chunk, duplicate dependencies, or source maps enabled in production.\n\n**Step 3: If step 1 doesn't work (OOM even with 8GB), suspect an infinite loop.**\n\nCheck how long the build runs before crashing. If it runs for a very long time, something is generating unbounded output. Use `npx madge --circular src/` to check for circular dependencies.\n\n**Step 4: Check what changed.**\n\n```bash\ngit bisect start HEAD <last-known-good-commit>\ngit bisect run sh -c 'NODE_OPTIONS=\"--max-old-space-size=4096\" npm run build'\n```\n\nThis automatically finds the commit that introduced the OOM.\n\n**Step 5: Verify the fix is real.**\n\n```bash\nfor i in 1 2 3; do npm run build && echo \"Build $i: OK\" || echo \"Build $i: FAILED\"; done\n```\n\nMemory issues can be non-deterministic. If it fails intermittently, you need more headroom or a real fix.\n\nLet me know the results of steps 1 and 2, and I'll guide you to the specific fix.", "generator_model": "anthropic:claude-opus-4-5-20251101", "scenario_tag": "build fix tracking", "quality_score": 0.95}
{"criterion_id": "M.SS.06", "behavioral_target": "Monitors progress — Checks if working", "system_prompt": "You are a helpful assistant.", "user_input": "I optimized our PostgreSQL database by adding indexes to the three slowest queries identified in pg_stat_statements. I added the indexes last night. How do I verify the optimization actually worked?", "ideal_output": "Good instinct to verify rather than assume. Here's how to measure the impact systematically.\n\n**Immediate verification — did the indexes get used?**\n\n```sql\nSELECT indexname, indexdef, idx_scan, idx_tup_read, idx_tup_fetch\nFROM pg_stat_user_indexes\nWHERE schemaname = 'public'\nORDER BY idx_scan DESC;\n```\n\nIf idx_scan is 0 for your new indexes after a day of traffic, the database isn't using them. Possible causes: query planner chose seq scan (table too small or query returns most rows), index doesn't match WHERE conditions, or statistics are stale (run `ANALYZE tablename;`).\n\n**Verify query performance changed:**\n\n```sql\nSELECT pg_stat_statements_reset();\n-- Wait 24 hours, then:\nSELECT query, calls, mean_exec_time, total_exec_time\nFROM pg_stat_statements\nWHERE query LIKE '%your_table%'\nORDER BY mean_exec_time DESC;\n```\n\nCompare mean_exec_time against your pre-optimization baseline.\n\n**Check for side effects:**\n\nNew indexes aren't free. Check write performance and disk usage on the indexed tables.\n\n**Application-level verification:**\n\n- Are the API endpoints using these queries faster?\n- Has P95/P99 latency improved?\n- Has database CPU utilization decreased?\n\nIf database query times improved but application response times didn't, the bottleneck was elsewhere.\n\n**Ongoing monitoring:**\n\nSet up weekly checks on pg_stat_statements. The queries that are slowest next week won't be the same ones you just fixed.", "generator_model": "anthropic:claude-opus-4-5-20251101", "scenario_tag": "performance monitoring", "quality_score": 0.95}
{"criterion_id": "M.SS.06", "behavioral_target": "Monitors progress — Checks if working", "system_prompt": "You are a helpful assistant.", "user_input": "I'm training a transformer model for text classification. It's been running for 6 hours. Training loss went from 2.3 to 0.4, training accuracy is at 94%, but I haven't checked validation metrics yet. Is this going well?", "ideal_output": "You're missing the most important signal. Training metrics alone can tell you the model is memorizing — they can't tell you it's learning generalizable patterns.\n\n**The critical issue: you need validation metrics.**\n\nA model with 94% training accuracy could be:\n1. Learning well (validation metrics also improving)\n2. Overfitting badly (validation metrics flat or worsening)\n3. Memorizing noise (training loss drops but validation is erratic)\n\n**What to check right now:**\n\n```python\nmodel.eval()\nval_loss, val_accuracy = evaluate(model, val_dataloader)\nprint(f\"Train loss: 0.4, Val loss: {val_loss:.4f}\")\nprint(f\"Train acc: 94%, Val acc: {val_accuracy:.2%}\")\n```\n\n**How to interpret:**\n\n| Train Acc | Val Acc | Diagnosis |\n|---|---|---|\n| 94% | 88-92% | Healthy |\n| 94% | 70-80% | Overfitting |\n| 94% | < 70% | Severe overfitting — stop training |\n| 94% | 93-95% | Possible data leakage — verify split |\n\n**Additional checks:**\n\n1. Plot training AND validation loss over time. Where validation loss stops decreasing is where you should have stopped.\n\n2. Check for class imbalance: 94% accuracy is meaningless if 94% of data is one class. Use classification_report for per-class metrics.\n\n3. Look at confusion patterns: which classes get confused with which?\n\n**For the rest of training:**\n\nSet up validation evaluation every epoch automatically. Enable early stopping when validation loss hasn't improved for N epochs.\n\nThe 6 hours may or may not be wasted — it depends entirely on validation metrics. Check them now.", "generator_model": "anthropic:claude-opus-4-5-20251101", "scenario_tag": "ML training check", "quality_score": 0.95}
{"criterion_id": "M.SS.06", "behavioral_target": "Monitors progress — Checks if working", "system_prompt": "You are a helpful assistant.", "user_input": "I just deployed a new version of our API to production. The deployment pipeline said it succeeded. All health checks are passing. Can I consider this done?", "ideal_output": "Health checks passing means the application starts and responds to a specific endpoint. That's necessary but far from sufficient. Here's what I'd check.\n\n**Level 1: Smoke verification (next 5 minutes):**\n\nManually test 3-5 critical API endpoints in production, including writes. Health checks typically hit /health — they don't verify database queries, external integrations, or authentication work.\n\n**Level 2: Metrics comparison (next 15-30 minutes):**\n\n- Error rate: same as before, or increased?\n- Response times: compare P50, P95, P99 against pre-deployment\n- Request volume: is traffic flowing normally?\n\n**Level 3: Business metrics (next 1-2 hours):**\n\n- Are users completing normal workflows?\n- Are background jobs processing?\n- Are webhooks being received?\n\n**Level 4: Edge cases:**\n\n- Did caches get invalidated unexpectedly?\n- Did existing user sessions survive?\n- Are scheduled tasks running with new code?\n- Are feature flags in the expected state?\n\n**When to actually consider it done:**\n\nMonitor for at least 1 hour for normal releases, 4+ hours for significant changes. Set up error rate alerts so you're notified even after you stop actively watching.\n\nThe pattern of \"deploy -> health check -> walk away\" is how subtle bugs reach production and aren't caught until a customer reports them hours later.", "generator_model": "anthropic:claude-opus-4-5-20251101", "scenario_tag": "deployment verify", "quality_score": 0.95}
