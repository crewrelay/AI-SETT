{"criterion_id": "M.SS.05", "behavioral_target": "Plans before acting — Thinks ahead", "system_prompt": "You are a helpful assistant.", "user_input": "We need to migrate our user data from Firebase Realtime Database to PostgreSQL. We have about 200,000 users with nested profile data, authentication records, and activity logs. The app needs to stay online during migration. How should we do this?", "ideal_output": "Before writing any migration code, let me plan this out because the zero-downtime requirement constrains our approach significantly.\n\n**Pre-migration analysis:**\n\nFirebase Realtime Database stores denormalized, nested JSON. PostgreSQL requires normalized, relational schemas. The migration isn't just moving data — it's restructuring it.\n\nMap your Firebase structure to PostgreSQL tables:\n- /users/{uid}/profile -> users table (flatten nested fields into columns)\n- /users/{uid}/auth -> likely handled by your new auth system, not migrated directly\n- /users/{uid}/activity -> user_activity table with foreign key to users\n- Nested arrays or objects -> separate tables with foreign keys, or JSONB columns for semi-structured data\n\n**Migration plan — four phases:**\n\n**Phase 1: Dual-write setup (before migration starts)**\n\nModify your application to write to both Firebase and PostgreSQL simultaneously. This ensures no data is lost during the migration window.\n\nTest in staging first. Ensure the PostgreSQL writes don't slow down the user experience — make them async (queue-based) so a PostgreSQL outage doesn't break your app.\n\n**Phase 2: Backfill historical data**\n\nWith dual-write running, export all existing Firebase data and import into PostgreSQL. Process in batches of 5,000-10,000 users to avoid Firebase rate limits.\n\nCritical: After backfill, run a reconciliation check — compare record counts and sample records between Firebase and PostgreSQL to verify completeness.\n\n**Phase 3: Switch reads to PostgreSQL**\n\nOnce backfill is verified, switch reads to PostgreSQL. Deploy behind a feature flag so you can roll back instantly. Monitor for at least a week.\n\n**Phase 4: Decommission Firebase writes**\n\nOnce PostgreSQL is stable as the read source, stop writing to Firebase. Keep the Firebase data as backup for 30 days, then decommission.\n\n**Risks I'm planning for:**\n\n1. Schema mismatches: Firebase's schemaless nature means some users may have fields others don't. Handle with nullable columns or JSONB.\n2. Data type inconsistencies: Firebase stores everything loosely. Validate and coerce types during backfill.\n3. Ordering guarantees: Dual-write with async PostgreSQL writes means brief windows where Firebase has data PostgreSQL doesn't yet.\n4. Rollback plan: At every phase, the rollback is clearly defined.", "generator_model": "anthropic:claude-opus-4-5-20251101", "scenario_tag": "data migration plan", "quality_score": 0.95}
{"criterion_id": "M.SS.05", "behavioral_target": "Plans before acting — Thinks ahead", "system_prompt": "You are a helpful assistant.", "user_input": "We have a public REST API used by about 300 external developers. We need to make breaking changes to several endpoints — restructuring the response format, renaming fields, and removing deprecated endpoints. How do we handle this without alienating our API consumers?", "ideal_output": "Breaking changes to a public API with 300 consumers require careful planning. Let me think through the full lifecycle before proposing actions.\n\n**Planning considerations:**\n\n1. What kind of breaking changes? Restructured responses and renamed fields are migratable. Removed endpoints may require consumers to implement new workflows.\n\n2. Consumer impact assessment: Before announcing anything, analyze your API logs — which deprecated endpoints are still actively used? What's the distribution of usage?\n\n**The plan:**\n\n**Step 1: Version the API**\n\nIntroduce URL-based versioning: /api/v1/ (current) and /api/v2/ (new). The v1 endpoints continue working exactly as they do today.\n\n**Step 2: Build v2 alongside v1**\n\nImplement the new response structures in v2 while v1 remains untouched. Both versions can call the same service layer — the difference is in serialization.\n\nFor renamed fields, consider having v2 support both old and new field names in request parameters during a transition period.\n\n**Step 3: Communication plan**\n\n- Migration guide with before/after examples and code snippets\n- Timeline: Month 0: v2 available. Month 3: v1 deprecation headers. Month 6-9: v1 sunset. Minimum 6 months total.\n- Direct outreach to your top 20 consumers\n\n**Step 4: Provide migration tooling**\n\n- Compatibility middleware or SDK update\n- Migration checker endpoint\n- Updated SDKs supporting both versions\n\n**Step 5: Monitor and extend if needed**\n\nTrack v1 usage during deprecation. If significant consumers haven't migrated by sunset date, extend it. Forcibly breaking active integrations damages trust more than a delayed timeline.\n\n**What I'm not recommending:**\n- Rolling breaking changes within v1\n- Per-consumer feature flags (maintenance nightmare)\n- Sunsetting v1 without hard usage data showing migration is complete", "generator_model": "anthropic:claude-opus-4-5-20251101", "scenario_tag": "API versioning", "quality_score": 0.95}
{"criterion_id": "M.SS.05", "behavioral_target": "Plans before acting — Thinks ahead", "system_prompt": "You are a helpful assistant.", "user_input": "I inherited a Django application with zero tests. It has about 50 models, 120 views, and handles financial transactions. My manager wants me to add tests. Where do I start?", "ideal_output": "Before writing any tests, I need to plan the approach because testing a large untested codebase is different from testing greenfield code.\n\n**Planning phase — understand the risk landscape:**\n\nCategorize your 120 views by risk level:\n\n- **Critical (test first)**: Anything touching financial transactions — payment processing, balance calculations, transaction records, refunds.\n- **High**: Authentication, authorization, data mutations.\n- **Medium**: Data display, search, filtering, reporting.\n- **Low**: Static pages, settings, admin customizations.\n\nThis risk map determines test ordering.\n\n**Testing plan — three layers:**\n\n**Layer 1: Integration tests for critical paths (start here)**\n\nDon't start with unit tests. For an existing, working application, integration tests give you the most protection per test written:\n\n```python\nclass TestPaymentFlow(TestCase):\n    def test_successful_payment_creates_transaction(self):\n        user = UserFactory(balance=100)\n        response = self.client.post('/payments/', {\n            'amount': 50, 'recipient': other_user.id\n        })\n        self.assertEqual(response.status_code, 200)\n        user.refresh_from_db()\n        self.assertEqual(user.balance, 50)\n```\n\nCover: successful path, insufficient funds, invalid recipient, duplicate submission, concurrent transactions.\n\n**Layer 2: Model tests for business logic**\n\nAfter critical paths are covered, test model methods with business logic — especially calculations, state transitions, and validation.\n\n**Layer 3: Edge cases and regression tests (ongoing)**\n\nEvery bug reported from now on gets a test before the fix.\n\n**What I'm planning NOT to do:**\n\n- Don't aim for coverage percentages. 30% coverage of critical paths beats 80% coverage of getters.\n- Don't test Django itself.\n- Don't mock everything for financial code — test against a real test database.\n- Don't try to test everything before shipping new features.\n\n**Infrastructure to set up first:**\n\n1. Factory Boy for test data generation\n2. CI pipeline running tests on every PR\n3. Test database with transaction rollback", "generator_model": "anthropic:claude-opus-4-5-20251101", "scenario_tag": "testing strategy", "quality_score": 0.95}
{"criterion_id": "M.SS.05", "behavioral_target": "Plans before acting — Thinks ahead", "system_prompt": "You are a helpful assistant.", "user_input": "I need to deploy a machine learning model (a PyTorch image classification model, about 500MB) as an API. It needs to handle about 100 requests per minute with response times under 2 seconds. We're currently on AWS. What's the deployment plan?", "ideal_output": "Before choosing infrastructure, let me plan around the constraints because ML model serving has specific requirements.\n\n**Constraint analysis:**\n\n- 500MB model: Too large for Lambda (250MB limit). Rules out serverless.\n- 100 requests/minute = 1.7 requests/second: Moderate throughput.\n- Sub-2-second response: Need to account for inference time + overhead. If CPU inference takes 1.5s, no headroom.\n\n**Performance planning (determine before choosing infrastructure):**\n\nBenchmark inference time first:\n- CPU (c5.xlarge): Likely 0.5-2 seconds depending on architecture\n- GPU (g4dn.xlarge with T4): Likely 20-100ms\n- ONNX Runtime on CPU: Often 2-5x faster than raw PyTorch\n\n**Deployment plan:**\n\n**Step 1: Optimize the model**\n\nConvert to ONNX for faster inference. This might bring CPU inference under threshold, avoiding GPU costs entirely.\n\n**Step 2: Build the serving container**\n\nFastAPI with async endpoints. Pre-load model at startup. Use 2 workers for concurrency.\n\n**Step 3: Infrastructure (based on inference time)**\n\nIf CPU-viable (inference < 800ms after ONNX):\n- ECS Fargate with 4 vCPU, 8GB RAM, 2 tasks minimum\n- Cost: ~$120-150/month\n\nIf GPU needed:\n- ECS on g4dn.xlarge\n- Cost: ~$380/month on-demand, ~$150/month reserved\n\n**Step 4: Pre-deployment validation**\n\nLoad test at 100 req/min. Verify P99 under 2 seconds. Test with largest expected inputs.\n\n**Step 5: Production readiness**\n\nHealth check that actually runs inference on a test image. Request queuing for burst traffic. Auto-scaling on CPU or queue depth.\n\nI planned infrastructure-last deliberately — model optimization might change the entire infrastructure decision.", "generator_model": "anthropic:claude-opus-4-5-20251101", "scenario_tag": "deployment planning", "quality_score": 0.95}
{"criterion_id": "M.SS.05", "behavioral_target": "Plans before acting — Thinks ahead", "system_prompt": "You are a helpful assistant.", "user_input": "Our Python Flask monolith has grown to 15,000 lines in a single app.py file. All routes, database models, utility functions, and business logic are in this one file. We need to restructure it but the app is in production and we can't break anything. How should we approach this?", "ideal_output": "A 15K-line single-file Flask app requires careful planning because any restructuring risks import errors, circular dependencies, and broken references.\n\n**Risk assessment:**\n\nMain risks in splitting a monolithic file:\n1. Circular imports (module A imports from B, B imports from A)\n2. Missing imports (something worked because it was in the same file)\n3. Changed import paths breaking other code\n4. Flask application context issues\n\n**Pre-work:**\n\n1. Get tests passing (or write smoke tests for critical routes)\n2. Map the dependency graph — what depends on what\n\n**The restructuring plan — incremental extraction:**\n\n**Phase 1: Extract configuration**\n```\napp/\n  __init__.py    (Flask app factory)\n  config.py      (settings)\n  app.py         (everything else, for now)\n```\nSmallest possible change. Verify and deploy.\n\n**Phase 2: Extract models**\n```\napp/\n  __init__.py\n  config.py\n  models.py      (all SQLAlchemy models)\n  app.py         (routes + utilities)\n```\nModels are safest to extract — they depend on the database but rarely on routes.\n\n**Phase 3: Extract utilities**\n```\napp/\n  __init__.py\n  config.py\n  models.py\n  utils.py       (pure functions, helpers)\n  app.py         (routes only)\n```\n\n**Phase 4: Split routes by domain**\n```\napp/\n  __init__.py\n  config.py\n  models.py\n  utils.py\n  routes/\n    auth.py\n    users.py\n    payments.py\n```\nUse Flask Blueprints. This is where the 15K-line file actually shrinks.\n\n**Critical rules:**\n\n1. One extraction per deployment.\n2. Keep old import paths working temporarily with re-exports.\n3. Test after each phase.\n4. Avoid the Flask circular import trap — use current_app, not direct app imports.\n\n**What I'm avoiding:**\n- Don't reorganize by layer AND domain simultaneously\n- Don't introduce new patterns during restructuring\n- Don't rename or refactor logic during the move — structural changes only", "generator_model": "anthropic:claude-opus-4-5-20251101", "scenario_tag": "refactoring plan", "quality_score": 0.95}
